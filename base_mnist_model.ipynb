{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size = 3,stride=1, padding=1)\n",
    "        # kernel_size = 5 means 5x5 filter\n",
    "        # kernel_size is the size of the filter window\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 3,stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = nn.functional.relu(nn.functional.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # view() is a reshape function\n",
    "        # -1 means the size of that dimension is inferred\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim = 1)\n",
    "        # log_softmax is a log of softmax function\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train = True, download = True,\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5), (0.5))\n",
    "                    ])),\n",
    "    batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train = False, download = True,\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5), (0.5))\n",
    "                    ])),\n",
    "    batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalilovm/projects/ML/venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.19823521202199781\n",
      "Epoch 2 - Training loss: 0.05685417905304906\n",
      "Epoch 3 - Training loss: 0.03896654491591553\n",
      "Epoch 4 - Training loss: 0.03113077980768271\n",
      "Epoch 5 - Training loss: 0.025261576023791903\n",
      "Epoch 6 - Training loss: 0.020136459946315532\n",
      "Epoch 7 - Training loss: 0.01520303558696112\n",
      "Epoch 8 - Training loss: 0.012412520682107431\n",
      "Epoch 9 - Training loss: 0.010962516023520564\n",
      "Epoch 10 - Training loss: 0.010835805355487452\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Training loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.12999725341797%\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.inference_mode():\n",
    "\t\tfor images, labels in test_loader:\n",
    "\t\t\t\toutputs = model(images)\n",
    "\t\t\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\t\t\ttotal += labels.size(0)\n",
    "\t\t\t\tcorrect += (predicted == labels).sum()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "# print sample shape\n",
    "print(train_loader.dataset.data.shape)\n",
    "print(train_loader.dataset.targets.shape)\n",
    "# which dimension should be image to predict?\n",
    "# 0: batch, 1: channel, 2: height, 3: width\n",
    "# example:\n",
    "# 0: 64, 1: 1, 2: 28, 3: 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "print(images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa3klEQVR4nO3de2zV9f3H8ddpaQ8g7amltKeViy0IZCI1ItRGZRoa2m4hcskCzmS4GQlYzABvq5ug25JOlizGhajJNpmZV7YB0ywsWGybbS0OlBEzVinp1hLacll6DhQo0H5+f/DzbEda4Hs4h3cvz0fySeg530/P2+++48npORx8zjknAACusyTrAQAAwxMBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJkZYD/Blvb29OnLkiNLS0uTz+azHAQB45JzTyZMnlZeXp6Sk/p/nDLgAHTlyRBMmTLAeAwBwjVpbWzV+/Ph+7x9wP4JLS0uzHgEAEAdX+v08YQHatGmTbr75Zo0cOVJFRUX6+OOPr2ofP3YDgKHhSr+fJyRA7777rtatW6cNGzbok08+UWFhoUpLS3X06NFEPBwAYDByCTBnzhxXUVER+bqnp8fl5eW5qqqqK+4NhUJOEovFYrEG+QqFQpf9/T7uz4DOnTunvXv3qqSkJHJbUlKSSkpKVF9ff8nx3d3dCofDUQsAMPTFPUDHjx9XT0+PcnJyom7PyclRe3v7JcdXVVUpEAhEFu+AA4DhwfxdcJWVlQqFQpHV2tpqPRIA4DqI+98DysrKUnJysjo6OqJu7+joUDAYvOR4v98vv98f7zEAAANc3J8BpaamatasWaquro7c1tvbq+rqahUXF8f74QAAg1RCPglh3bp1Wr58ue68807NmTNHL730krq6uvTtb387EQ8HABiEEhKgpUuX6tixY1q/fr3a29t1++23a8eOHZe8MQEAMHz5nHPOeoj/FQ6HFQgErMcAAFyjUCik9PT0fu83fxccAGB4IkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZGWA8AXMmIEd4v09zc3JgeKynJ+5/JfD5fTI/l1blz5zzvSU1Njemxjh8/7nnPqVOnYnosDF88AwIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPBhpLiupkyZ4nnPL37xC897Zs+e7XmPFNsHn8byYaTOOc97Lly44HlPSkqK5z2SdPjwYc97du7c6XnP+vXrPe/p6OjwvAcDE8+AAAAmCBAAwETcA/T888/L5/NFrenTp8f7YQAAg1xCXgO69dZb9eGHH/73QWL4uToAYGhLSBlGjBihYDCYiG8NABgiEvIa0MGDB5WXl6eCggI99NBDamlp6ffY7u5uhcPhqAUAGPriHqCioiJt3rxZO3bs0CuvvKLm5mbde++9OnnyZJ/HV1VVKRAIRNaECRPiPRIAYACKe4DKy8v1jW98QzNnzlRpaan++Mc/qrOzU++9916fx1dWVioUCkVWa2trvEcCAAxACX93QEZGhqZOnaqmpqY+7/f7/fL7/YkeAwAwwCT87wGdOnVKhw4dUm5ubqIfCgAwiMQ9QE8++aRqa2v1r3/9S3/961+1aNEiJScn68EHH4z3QwEABrG4/wju8OHDevDBB3XixAmNGzdO99xzjxoaGjRu3Lh4PxQAYBDzuVg+FTGBwuGwAoGA9Ri4ClOnTvW8Z8eOHZ735Ofne97T3t7ueY8kHT9+3POeWP6idSz/t+vu7va8Z/To0Z73SFJBQYHnPbGch40bN3re88wzz3jeAxuhUEjp6en93s9nwQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJhL+D9Jh6Pr+97/veU8sHyy6c+dOz3u+9a1ved4jSceOHfO8Jynp+vw57sKFC573pKSkxPRYd9xxh+c9sVwPdXV1nvdg6OAZEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEz4nHPOeoj/FQ6HFQgErMcYVsaNGxfTvgMHDnjeM2bMGM97ZsyY4XlPU1OT5z0A4isUCik9Pb3f+3kGBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGE9AOxNmzYtpn1jx471vOeTTz7xvKetrc3zHgADH8+AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATfBgplJGRcd0ea+rUqZ73/O1vf/O85zvf+Y7nPZLU0tLieU9ycrLnPT09PZ73hEIhz3vOnDnjeY8k9fb2xrQP8IJnQAAAEwQIAGDCc4Dq6uq0YMEC5eXlyefzadu2bVH3O+e0fv165ebmatSoUSopKdHBgwfjNS8AYIjwHKCuri4VFhZq06ZNfd6/ceNGvfzyy3r11Ve1e/du3XDDDSotLdXZs2eveVgAwNDh+U0I5eXlKi8v7/M+55xeeukl/eAHP9ADDzwgSXrjjTeUk5Ojbdu2admyZdc2LQBgyIjra0DNzc1qb29XSUlJ5LZAIKCioiLV19f3uae7u1vhcDhqAQCGvrgGqL29XZKUk5MTdXtOTk7kvi+rqqpSIBCIrAkTJsRzJADAAGX+LrjKykqFQqHIam1ttR4JAHAdxDVAwWBQktTR0RF1e0dHR+S+L/P7/UpPT49aAIChL64Bys/PVzAYVHV1deS2cDis3bt3q7i4OJ4PBQAY5Dy/C+7UqVNqamqKfN3c3Kx9+/YpMzNTEydO1Jo1a/TjH/9Yt9xyi/Lz8/Xcc88pLy9PCxcujOfcAIBBznOA9uzZo/vvvz/y9bp16yRJy5cv1+bNm/X000+rq6tLK1asUGdnp+655x7t2LFDI0eOjN/UAIBBz+ecc9ZD/K9wOKxAIGA9xrCyYMGCmPb94Q9/iPMk8XPhwoWY9sXyF6ZHjPD+mb6xzPef//zH856jR4963iNJr732muc9v/rVrzzv4UNPh7ZQKHTZ1/XN3wUHABieCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML7x/hiyDl27FhM+2L5JOOkJO9/5vn888897+nq6vK8J1bJycme94wZM8bzntzcXM97Jk6c6HmPJN15552e93z5X0K+Gu+//77nPRg6eAYEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgw0ihPXv2xLTv448/9rznrrvu8rznd7/7nec9zz77rOc9A93YsWM97/ntb38b02Pdd999nvfk5OTE9FgYvngGBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4MNIoQsXLsS0L5YP/PzTn/7kec/3vvc9z3sKCgo875Gkjo4Oz3s+//xzz3u6uro87/H7/Z73fOUrX/G8R4rtmmhoaIjpsTB88QwIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDhc8456yH+VzgcViAQsB4DCfLEE0943vPiiy963pOcnOx5D/4rlg8Wvffeez3vifWDcDE4hEIhpaen93s/z4AAACYIEADAhOcA1dXVacGCBcrLy5PP59O2bdui7n/44Yfl8/miVllZWbzmBQAMEZ4D1NXVpcLCQm3atKnfY8rKytTW1hZZb7/99jUNCQAYejz/i6jl5eUqLy+/7DF+v1/BYDDmoQAAQ19CXgOqqalRdna2pk2bplWrVunEiRP9Htvd3a1wOBy1AABDX9wDVFZWpjfeeEPV1dV68cUXVVtbq/LycvX09PR5fFVVlQKBQGRNmDAh3iMBAAYgzz+Cu5Jly5ZFfn3bbbdp5syZmjx5smpqajRv3rxLjq+srNS6desiX4fDYSIEAMNAwt+GXVBQoKysLDU1NfV5v9/vV3p6etQCAAx9CQ/Q4cOHdeLECeXm5ib6oQAAg4jnH8GdOnUq6tlMc3Oz9u3bp8zMTGVmZuqFF17QkiVLFAwGdejQIT399NOaMmWKSktL4zo4AGBw8xygPXv26P777498/cXrN8uXL9crr7yi/fv369e//rU6OzuVl5en+fPn60c/+pH8fn/8pgYADHp8GCkGvNtvv93znmnTpsX0WLG8BhnLY8XyB7IRI7y/Z+j8+fOe90jSL3/5S897/v73v8f0WBi6+DBSAMCARIAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABN8GjYAICH4NGwAwIBEgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMOEpQFVVVZo9e7bS0tKUnZ2thQsXqrGxMeqYs2fPqqKiQmPHjtWYMWO0ZMkSdXR0xHVoAMDg5ylAtbW1qqioUENDg3bu3Knz589r/vz56urqihyzdu1avf/++9qyZYtqa2t15MgRLV68OO6DAwAGOXcNjh496iS52tpa55xznZ2dLiUlxW3ZsiVyzIEDB5wkV19ff1XfMxQKOUksFovFGuQrFApd9vf7a3oNKBQKSZIyMzMlSXv37tX58+dVUlISOWb69OmaOHGi6uvr+/we3d3dCofDUQsAMPTFHKDe3l6tWbNGd999t2bMmCFJam9vV2pqqjIyMqKOzcnJUXt7e5/fp6qqSoFAILImTJgQ60gAgEEk5gBVVFTos88+0zvvvHNNA1RWVioUCkVWa2vrNX0/AMDgMCKWTatXr9YHH3yguro6jR8/PnJ7MBjUuXPn1NnZGfUsqKOjQ8FgsM/v5ff75ff7YxkDADCIeXoG5JzT6tWrtXXrVu3atUv5+flR98+aNUspKSmqrq6O3NbY2KiWlhYVFxfHZ2IAwJDg6RlQRUWF3nrrLW3fvl1paWmR13UCgYBGjRqlQCCgRx55ROvWrVNmZqbS09P1+OOPq7i4WHfddVdC/gMAAIOUl7ddq5+32r3++uuRY86cOeMee+wxd+ONN7rRo0e7RYsWuba2tqt+DN6GzWKxWENjXelt2L7/D8uAEQ6HFQgErMcAAFyjUCik9PT0fu/ns+AAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJTwGqqqrS7NmzlZaWpuzsbC1cuFCNjY1Rx9x3333y+XxRa+XKlXEdGgAw+HkKUG1trSoqKtTQ0KCdO3fq/Pnzmj9/vrq6uqKOe/TRR9XW1hZZGzdujOvQAIDBb4SXg3fs2BH19ebNm5Wdna29e/dq7ty5kdtHjx6tYDAYnwkBAEPSNb0GFAqFJEmZmZlRt7/55pvKysrSjBkzVFlZqdOnT/f7Pbq7uxUOh6MWAGAYcDHq6elxX//6193dd98ddftrr73mduzY4fbv3+9+85vfuJtuusktWrSo3++zYcMGJ4nFYrFYQ2yFQqHLdiTmAK1cudJNmjTJtba2Xva46upqJ8k1NTX1ef/Zs2ddKBSKrNbWVvOTxmKxWKxrX1cKkKfXgL6wevVqffDBB6qrq9P48eMve2xRUZEkqampSZMnT77kfr/fL7/fH8sYAIBBzFOAnHN6/PHHtXXrVtXU1Cg/P/+Ke/bt2ydJys3NjWlAAMDQ5ClAFRUVeuutt7R9+3alpaWpvb1dkhQIBDRq1CgdOnRIb731lr72ta9p7Nix2r9/v9auXau5c+dq5syZCfkPAAAMUl5e91E/P+d7/fXXnXPOtbS0uLlz57rMzEzn9/vdlClT3FNPPXXFnwP+r1AoZP5zSxaLxWJd+7rS7/2+/w/LgBEOhxUIBKzHAABco1AopPT09H7v57PgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmBlyAnHPWIwAA4uBKv58PuACdPHnSegQAQBxc6fdznxtgTzl6e3t15MgRpaWlyefzRd0XDoc1YcIEtba2Kj093WhCe5yHizgPF3EeLuI8XDQQzoNzTidPnlReXp6Skvp/njPiOs50VZKSkjR+/PjLHpOenj6sL7AvcB4u4jxcxHm4iPNwkfV5CAQCVzxmwP0IDgAwPBAgAICJQRUgv9+vDRs2yO/3W49iivNwEefhIs7DRZyHiwbTeRhwb0IAAAwPg+oZEABg6CBAAAATBAgAYIIAAQBMDJoAbdq0STfffLNGjhypoqIiffzxx9YjXXfPP/+8fD5f1Jo+fbr1WAlXV1enBQsWKC8vTz6fT9u2bYu63zmn9evXKzc3V6NGjVJJSYkOHjxoM2wCXek8PPzww5dcH2VlZTbDJkhVVZVmz56ttLQ0ZWdna+HChWpsbIw65uzZs6qoqNDYsWM1ZswYLVmyRB0dHUYTJ8bVnIf77rvvkuth5cqVRhP3bVAE6N1339W6deu0YcMGffLJJyosLFRpaamOHj1qPdp1d+utt6qtrS2y/vznP1uPlHBdXV0qLCzUpk2b+rx/48aNevnll/Xqq69q9+7duuGGG1RaWqqzZ89e50kT60rnQZLKysqiro+33377Ok6YeLW1taqoqFBDQ4N27typ8+fPa/78+erq6oocs3btWr3//vvasmWLamtrdeTIES1evNhw6vi7mvMgSY8++mjU9bBx40ajifvhBoE5c+a4ioqKyNc9PT0uLy/PVVVVGU51/W3YsMEVFhZaj2FKktu6dWvk697eXhcMBt1Pf/rTyG2dnZ3O7/e7t99+22DC6+PL58E555YvX+4eeOABk3msHD161ElytbW1zrmL/9unpKS4LVu2RI45cOCAk+Tq6+utxky4L58H55z76le/6r773e/aDXUVBvwzoHPnzmnv3r0qKSmJ3JaUlKSSkhLV19cbTmbj4MGDysvLU0FBgR566CG1tLRYj2SqublZ7e3tUddHIBBQUVHRsLw+ampqlJ2drWnTpmnVqlU6ceKE9UgJFQqFJEmZmZmSpL179+r8+fNR18P06dM1ceLEIX09fPk8fOHNN99UVlaWZsyYocrKSp0+fdpivH4NuA8j/bLjx4+rp6dHOTk5Ubfn5OTon//8p9FUNoqKirR582ZNmzZNbW1teuGFF3Tvvffqs88+U1pamvV4Jtrb2yWpz+vji/uGi7KyMi1evFj5+fk6dOiQnn32WZWXl6u+vl7JycnW48Vdb2+v1qxZo7vvvlszZsyQdPF6SE1NVUZGRtSxQ/l66Os8SNI3v/lNTZo0SXl5edq/f7+eeeYZNTY26ve//73htNEGfIDwX+Xl5ZFfz5w5U0VFRZo0aZLee+89PfLII4aTYSBYtmxZ5Ne33XabZs6cqcmTJ6umpkbz5s0znCwxKioq9Nlnnw2L10Evp7/zsGLFisivb7vtNuXm5mrevHk6dOiQJk+efL3H7NOA/xFcVlaWkpOTL3kXS0dHh4LBoNFUA0NGRoamTp2qpqYm61HMfHENcH1cqqCgQFlZWUPy+li9erU++OADffTRR1H/fEswGNS5c+fU2dkZdfxQvR76Ow99KSoqkqQBdT0M+AClpqZq1qxZqq6ujtzW29ur6upqFRcXG05m79SpUzp06JByc3OtRzGTn5+vYDAYdX2Ew2Ht3r172F8fhw8f1okTJ4bU9eGc0+rVq7V161bt2rVL+fn5UffPmjVLKSkpUddDY2OjWlpahtT1cKXz0Jd9+/ZJ0sC6HqzfBXE13nnnHef3+93mzZvdP/7xD7dixQqXkZHh2tvbrUe7rp544glXU1Pjmpub3V/+8hdXUlLisrKy3NGjR61HS6iTJ0+6Tz/91H366adOkvvZz37mPv30U/fvf//bOefcT37yE5eRkeG2b9/u9u/f7x544AGXn5/vzpw5Yzx5fF3uPJw8edI9+eSTrr6+3jU3N7sPP/zQ3XHHHe6WW25xZ8+etR49blatWuUCgYCrqalxbW1tkXX69OnIMStXrnQTJ050u3btcnv27HHFxcWuuLjYcOr4u9J5aGpqcj/84Q/dnj17XHNzs9u+fbsrKChwc+fONZ482qAIkHPO/fznP3cTJ050qampbs6cOa6hocF6pOtu6dKlLjc316WmprqbbrrJLV261DU1NVmPlXAfffSRk3TJWr58uXPu4luxn3vuOZeTk+P8fr+bN2+ea2xstB06AS53Hk6fPu3mz5/vxo0b51JSUtykSZPco48+OuT+kNbXf78k9/rrr0eOOXPmjHvsscfcjTfe6EaPHu0WLVrk2tra7IZOgCudh5aWFjd37lyXmZnp/H6/mzJlinvqqadcKBSyHfxL+OcYAAAmBvxrQACAoYkAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMPF/uTmXluCNosQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] 9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from mnist_cnn_model import Net\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(image):\n",
    "    image = image.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image.convert('L')\n",
    "    image = image.resize((28, 28))\n",
    "    image = np.array(image)\n",
    "    image = image.reshape(1, 28, 28)\n",
    "    image = torch.from_numpy(image).float()\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def predict(image):\n",
    "    # image = preprocess_image(image)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return probabilities[0].tolist(), predicted.item()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    # image is test.png from .\n",
    "    image = Image.open(\"test.png\")\n",
    "    test_image = preprocess_image(image)\n",
    "    show_image(test_image)\n",
    "    probabilities, predicted = predict(test_image)\n",
    "    print(probabilities, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
